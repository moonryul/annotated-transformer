{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeol/anaconda3/envs/Seq2SeqAttn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "\n",
    "#############################\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "# import math\n",
    "# import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)] #MJ: add positioning embedding vector of x to the word empbedding vector x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "es, that's a great analogy! A stack of Transformer blocks is indeed analogous to a stack of convolutional layers in the way they both progressively transform the input data and extract more abstract features at each layer, though they operate on different types of data and use different mechanisms.\n",
    "\n",
    "Hereâ€™s a breakdown of the analogy:\n",
    "\n",
    "1. Input and Output:\n",
    "Transformer Stack: Takes a sequence of tokens as input and outputs a sequence of contextually enhanced representations (feature vectors), where each token is represented by a vector that captures its contextual relationships with other tokens.\n",
    "Convolutional Layers: Takes an image (2D grid of pixels) as input and outputs feature maps, where each map highlights certain features (like edges, textures, etc.) extracted from the image.\n",
    "2. Transformation Process:\n",
    "Transformer Stack: Each block in the stack applies self-attention (to model relationships between tokens) and feed-forward networks (to refine the features). As you go deeper in the stack, the model learns more abstract relationships between the tokens in the sequence.\n",
    "Convolutional Stack: Each layer applies convolutions (to capture local spatial relationships between pixels) and non-linear activations. Deeper layers learn more abstract and complex features, such as detecting objects or shapes from lower-level features like edges.\n",
    "3. Feature Representation:\n",
    "Transformers: The stack produces a sequence of feature vectors, where each vector corresponds to a token in the input sequence and represents a combination of its semantic and contextual meaning.\n",
    "Convolutional Layers: The stack produces a set of feature maps, where each map highlights different aspects of the image's structure or content (e.g., edges, textures, or complex patterns).\n",
    "4. Hierarchical Learning:\n",
    "In both architectures, earlier layers capture low-level information, while deeper layers capture higher-level, more abstract information:\n",
    "Transformer: Earlier layers might focus on shorter-range dependencies, while deeper layers capture long-range dependencies and complex relationships between tokens.\n",
    "Convolutions: Earlier layers might capture simple patterns like edges, while deeper layers identify more complex structures like objects.\n",
    "Key Difference:\n",
    "Spatial Relationships: Convolutions are designed to capture local spatial dependencies in images by focusing on nearby pixels, while Transformers capture global dependencies by allowing each token to attend to every other token in the sequence, regardless of their position.\n",
    "Summary:\n",
    "The analogy between a stack of Transformer blocks and a stack of convolutional layers works because both progressively transform the input to produce a more abstract and contextually enriched representation:\n",
    "\n",
    "Transformers produce a sequence of feature vectors that encode the relationships between tokens.\n",
    "Convolutional layers produce feature maps that highlight various aspects of an image.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "        #MJ: enc_output: [64,100,512]   \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "        #MJ: dec_output: torch.Size([64, 99, 512])\n",
    "        output = self.fc(dec_output)\n",
    "        return output #MJ: [64,99,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1749, 2867, 4651,  ..., 4130,  514,  410],\n",
       "        [ 236, 1746, 4941,  ..., 1872, 3643, 4555],\n",
       "        [ 705, 3274, 3992,  ..., 2960, 2967, 4016],\n",
       "        ...,\n",
       "        [4503, 4600, 4048,  ..., 1497, 1449, 3994],\n",
       "        [3688, 3933, 3771,  ..., 3511,  960, 4111],\n",
       "        [2265,  688, 3635,  ..., 1579, 3889, 2557]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.689992904663086\n",
      "Epoch: 2, Loss: 8.555461883544922\n",
      "Epoch: 3, Loss: 8.484436988830566\n",
      "Epoch: 4, Loss: 8.428950309753418\n",
      "Epoch: 5, Loss: 8.371879577636719\n",
      "Epoch: 6, Loss: 8.299415588378906\n",
      "Epoch: 7, Loss: 8.21992301940918\n",
      "Epoch: 8, Loss: 8.143165588378906\n",
      "Epoch: 9, Loss: 8.055954933166504\n",
      "Epoch: 10, Loss: 7.9785590171813965\n",
      "Epoch: 11, Loss: 7.904333114624023\n",
      "Epoch: 12, Loss: 7.811092376708984\n",
      "Epoch: 13, Loss: 7.729271411895752\n",
      "Epoch: 14, Loss: 7.6476054191589355\n",
      "Epoch: 15, Loss: 7.56585693359375\n",
      "Epoch: 16, Loss: 7.481439113616943\n",
      "Epoch: 17, Loss: 7.398675441741943\n",
      "Epoch: 18, Loss: 7.314747333526611\n",
      "Epoch: 19, Loss: 7.2287092208862305\n",
      "Epoch: 20, Loss: 7.145570755004883\n",
      "Epoch: 21, Loss: 7.068298816680908\n",
      "Epoch: 22, Loss: 6.998837471008301\n",
      "Epoch: 23, Loss: 6.919703483581543\n",
      "Epoch: 24, Loss: 6.847179889678955\n",
      "Epoch: 25, Loss: 6.768188953399658\n",
      "Epoch: 26, Loss: 6.692165374755859\n",
      "Epoch: 27, Loss: 6.6215434074401855\n",
      "Epoch: 28, Loss: 6.5354533195495605\n",
      "Epoch: 29, Loss: 6.473706245422363\n",
      "Epoch: 30, Loss: 6.402753829956055\n",
      "Epoch: 31, Loss: 6.33170223236084\n",
      "Epoch: 32, Loss: 6.2632341384887695\n",
      "Epoch: 33, Loss: 6.199820041656494\n",
      "Epoch: 34, Loss: 6.1275506019592285\n",
      "Epoch: 35, Loss: 6.061014652252197\n",
      "Epoch: 36, Loss: 5.995894432067871\n",
      "Epoch: 37, Loss: 5.9245381355285645\n",
      "Epoch: 38, Loss: 5.864166736602783\n",
      "Epoch: 39, Loss: 5.804820537567139\n",
      "Epoch: 40, Loss: 5.731954574584961\n",
      "Epoch: 41, Loss: 5.679481506347656\n",
      "Epoch: 42, Loss: 5.616442680358887\n",
      "Epoch: 43, Loss: 5.556369304656982\n",
      "Epoch: 44, Loss: 5.497838497161865\n",
      "Epoch: 45, Loss: 5.4247541427612305\n",
      "Epoch: 46, Loss: 5.369734764099121\n",
      "Epoch: 47, Loss: 5.310245513916016\n",
      "Epoch: 48, Loss: 5.253327369689941\n",
      "Epoch: 49, Loss: 5.2030720710754395\n",
      "Epoch: 50, Loss: 5.139216423034668\n",
      "Epoch: 51, Loss: 5.087149620056152\n",
      "Epoch: 52, Loss: 5.021050453186035\n",
      "Epoch: 53, Loss: 4.97015905380249\n",
      "Epoch: 54, Loss: 4.91398286819458\n",
      "Epoch: 55, Loss: 4.8627705574035645\n",
      "Epoch: 56, Loss: 4.796297550201416\n",
      "Epoch: 57, Loss: 4.7474517822265625\n",
      "Epoch: 58, Loss: 4.6973419189453125\n",
      "Epoch: 59, Loss: 4.647116184234619\n",
      "Epoch: 60, Loss: 4.594399452209473\n",
      "Epoch: 61, Loss: 4.536532402038574\n",
      "Epoch: 62, Loss: 4.484192848205566\n",
      "Epoch: 63, Loss: 4.441569805145264\n",
      "Epoch: 64, Loss: 4.388303279876709\n",
      "Epoch: 65, Loss: 4.3330979347229\n",
      "Epoch: 66, Loss: 4.2845587730407715\n",
      "Epoch: 67, Loss: 4.235659122467041\n",
      "Epoch: 68, Loss: 4.181856632232666\n",
      "Epoch: 69, Loss: 4.139230728149414\n",
      "Epoch: 70, Loss: 4.0884881019592285\n",
      "Epoch: 71, Loss: 4.040106773376465\n",
      "Epoch: 72, Loss: 3.9882090091705322\n",
      "Epoch: 73, Loss: 3.9402570724487305\n",
      "Epoch: 74, Loss: 3.8985509872436523\n",
      "Epoch: 75, Loss: 3.847181797027588\n",
      "Epoch: 76, Loss: 3.795170545578003\n",
      "Epoch: 77, Loss: 3.7371222972869873\n",
      "Epoch: 78, Loss: 3.7032463550567627\n",
      "Epoch: 79, Loss: 3.663184642791748\n",
      "Epoch: 80, Loss: 3.6108672618865967\n",
      "Epoch: 81, Loss: 3.5609312057495117\n",
      "Epoch: 82, Loss: 3.5198206901550293\n",
      "Epoch: 83, Loss: 3.470418691635132\n",
      "Epoch: 84, Loss: 3.42801570892334\n",
      "Epoch: 85, Loss: 3.379955291748047\n",
      "Epoch: 86, Loss: 3.335502862930298\n",
      "Epoch: 87, Loss: 3.2923521995544434\n",
      "Epoch: 88, Loss: 3.243563652038574\n",
      "Epoch: 89, Loss: 3.1964879035949707\n",
      "Epoch: 90, Loss: 3.1519105434417725\n",
      "Epoch: 91, Loss: 3.1090664863586426\n",
      "Epoch: 92, Loss: 3.060850143432617\n",
      "Epoch: 93, Loss: 3.026350736618042\n",
      "Epoch: 94, Loss: 2.9778473377227783\n",
      "Epoch: 95, Loss: 2.9376585483551025\n",
      "Epoch: 96, Loss: 2.888516902923584\n",
      "Epoch: 97, Loss: 2.848496913909912\n",
      "Epoch: 98, Loss: 2.8130455017089844\n",
      "Epoch: 99, Loss: 2.7664849758148193\n",
      "Epoch: 100, Loss: 2.730342388153076\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    #MJ: the input to the decoder (tgt_data[:, :-1]) should consist of all tokens up to the current token, \n",
    "    #     excluding the last token.\n",
    "    #MJ: output: [64,99,5000] => [6336, 5000] <==> [6336]\n",
    "    loss = criterion( output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1)) #MJ: 6336 = 64 * 99\n",
    "    #MJ: The prediction of <EOS> occurs because the model is trained on the target (tgt_data[:, 1:]), which includes <EOS>.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.823338508605957\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Seq2SeqAttn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
