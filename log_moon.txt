in greedy_decode: during encoding
in MultiheadAttention: Q=torch.Size([1, 128, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 128, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 128, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 128, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 128, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 128, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 128, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 128, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 128, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 128, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 128, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 128, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 1, 512]), V=torch.Size([1, 1, 512]), mask=torch.Size([1, 1, 1, 1])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 1]); mask:torch.Size([1, 1, 1, 1])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 1, 512]), V=torch.Size([1, 1, 512]), mask=torch.Size([1, 1, 1, 1])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 1]); mask:torch.Size([1, 1, 1, 1])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 1, 512]), V=torch.Size([1, 1, 512]), mask=torch.Size([1, 1, 1, 1])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 1]); mask:torch.Size([1, 1, 1, 1])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 1, 512]), V=torch.Size([1, 1, 512]), mask=torch.Size([1, 1, 1, 1])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 1]); mask:torch.Size([1, 1, 1, 1])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 1, 512]), V=torch.Size([1, 1, 512]), mask=torch.Size([1, 1, 1, 1])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 1]); mask:torch.Size([1, 1, 1, 1])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 1, 512]), V=torch.Size([1, 1, 512]), mask=torch.Size([1, 1, 1, 1])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 1]); mask:torch.Size([1, 1, 1, 1])
in MultiheadAttention: Q=torch.Size([1, 1, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 1, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 2, 512]), V=torch.Size([1, 2, 512]), mask=torch.Size([1, 1, 2, 2])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 2]); mask:torch.Size([1, 1, 2, 2])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 2, 512]), V=torch.Size([1, 2, 512]), mask=torch.Size([1, 1, 2, 2])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 2]); mask:torch.Size([1, 1, 2, 2])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 2, 512]), V=torch.Size([1, 2, 512]), mask=torch.Size([1, 1, 2, 2])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 2]); mask:torch.Size([1, 1, 2, 2])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 2, 512]), V=torch.Size([1, 2, 512]), mask=torch.Size([1, 1, 2, 2])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 2]); mask:torch.Size([1, 1, 2, 2])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 2, 512]), V=torch.Size([1, 2, 512]), mask=torch.Size([1, 1, 2, 2])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 2]); mask:torch.Size([1, 1, 2, 2])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 2, 512]), V=torch.Size([1, 2, 512]), mask=torch.Size([1, 1, 2, 2])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 2]); mask:torch.Size([1, 1, 2, 2])
in MultiheadAttention: Q=torch.Size([1, 2, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 2, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 3, 512]), V=torch.Size([1, 3, 512]), mask=torch.Size([1, 1, 3, 3])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 3]); mask:torch.Size([1, 1, 3, 3])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 3, 512]), V=torch.Size([1, 3, 512]), mask=torch.Size([1, 1, 3, 3])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 3]); mask:torch.Size([1, 1, 3, 3])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 3, 512]), V=torch.Size([1, 3, 512]), mask=torch.Size([1, 1, 3, 3])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 3]); mask:torch.Size([1, 1, 3, 3])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 3, 512]), V=torch.Size([1, 3, 512]), mask=torch.Size([1, 1, 3, 3])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 3]); mask:torch.Size([1, 1, 3, 3])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 3, 512]), V=torch.Size([1, 3, 512]), mask=torch.Size([1, 1, 3, 3])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 3]); mask:torch.Size([1, 1, 3, 3])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 3, 512]), V=torch.Size([1, 3, 512]), mask=torch.Size([1, 1, 3, 3])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 3]); mask:torch.Size([1, 1, 3, 3])
in MultiheadAttention: Q=torch.Size([1, 3, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 3, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 4, 512]), V=torch.Size([1, 4, 512]), mask=torch.Size([1, 1, 4, 4])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 4]); mask:torch.Size([1, 1, 4, 4])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 4, 512]), V=torch.Size([1, 4, 512]), mask=torch.Size([1, 1, 4, 4])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 4]); mask:torch.Size([1, 1, 4, 4])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 4, 512]), V=torch.Size([1, 4, 512]), mask=torch.Size([1, 1, 4, 4])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 4]); mask:torch.Size([1, 1, 4, 4])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 4, 512]), V=torch.Size([1, 4, 512]), mask=torch.Size([1, 1, 4, 4])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 4]); mask:torch.Size([1, 1, 4, 4])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 4, 512]), V=torch.Size([1, 4, 512]), mask=torch.Size([1, 1, 4, 4])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 4]); mask:torch.Size([1, 1, 4, 4])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 4, 512]), V=torch.Size([1, 4, 512]), mask=torch.Size([1, 1, 4, 4])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 4]); mask:torch.Size([1, 1, 4, 4])
in MultiheadAttention: Q=torch.Size([1, 4, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 4, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 5, 512]), V=torch.Size([1, 5, 512]), mask=torch.Size([1, 1, 5, 5])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 5]); mask:torch.Size([1, 1, 5, 5])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 5, 512]), V=torch.Size([1, 5, 512]), mask=torch.Size([1, 1, 5, 5])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 5]); mask:torch.Size([1, 1, 5, 5])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 5, 512]), V=torch.Size([1, 5, 512]), mask=torch.Size([1, 1, 5, 5])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 5]); mask:torch.Size([1, 1, 5, 5])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 5, 512]), V=torch.Size([1, 5, 512]), mask=torch.Size([1, 1, 5, 5])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 5]); mask:torch.Size([1, 1, 5, 5])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 5, 512]), V=torch.Size([1, 5, 512]), mask=torch.Size([1, 1, 5, 5])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 5]); mask:torch.Size([1, 1, 5, 5])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 5, 512]), V=torch.Size([1, 5, 512]), mask=torch.Size([1, 1, 5, 5])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 5]); mask:torch.Size([1, 1, 5, 5])
in MultiheadAttention: Q=torch.Size([1, 5, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 5, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 6, 512]), V=torch.Size([1, 6, 512]), mask=torch.Size([1, 1, 6, 6])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 6]); mask:torch.Size([1, 1, 6, 6])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 6, 512]), V=torch.Size([1, 6, 512]), mask=torch.Size([1, 1, 6, 6])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 6]); mask:torch.Size([1, 1, 6, 6])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 6, 512]), V=torch.Size([1, 6, 512]), mask=torch.Size([1, 1, 6, 6])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 6]); mask:torch.Size([1, 1, 6, 6])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 6, 512]), V=torch.Size([1, 6, 512]), mask=torch.Size([1, 1, 6, 6])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 6]); mask:torch.Size([1, 1, 6, 6])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 6, 512]), V=torch.Size([1, 6, 512]), mask=torch.Size([1, 1, 6, 6])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 6]); mask:torch.Size([1, 1, 6, 6])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 6, 512]), V=torch.Size([1, 6, 512]), mask=torch.Size([1, 1, 6, 6])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 6]); mask:torch.Size([1, 1, 6, 6])
in MultiheadAttention: Q=torch.Size([1, 6, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 6, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 7, 512]), V=torch.Size([1, 7, 512]), mask=torch.Size([1, 1, 7, 7])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 7]); mask:torch.Size([1, 1, 7, 7])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 7, 512]), V=torch.Size([1, 7, 512]), mask=torch.Size([1, 1, 7, 7])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 7]); mask:torch.Size([1, 1, 7, 7])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 7, 512]), V=torch.Size([1, 7, 512]), mask=torch.Size([1, 1, 7, 7])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 7]); mask:torch.Size([1, 1, 7, 7])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 7, 512]), V=torch.Size([1, 7, 512]), mask=torch.Size([1, 1, 7, 7])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 7]); mask:torch.Size([1, 1, 7, 7])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 7, 512]), V=torch.Size([1, 7, 512]), mask=torch.Size([1, 1, 7, 7])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 7]); mask:torch.Size([1, 1, 7, 7])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 7, 512]), V=torch.Size([1, 7, 512]), mask=torch.Size([1, 1, 7, 7])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 7]); mask:torch.Size([1, 1, 7, 7])
in MultiheadAttention: Q=torch.Size([1, 7, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 7, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 8, 512]), V=torch.Size([1, 8, 512]), mask=torch.Size([1, 1, 8, 8])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 8]); mask:torch.Size([1, 1, 8, 8])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 8, 512]), V=torch.Size([1, 8, 512]), mask=torch.Size([1, 1, 8, 8])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 8]); mask:torch.Size([1, 1, 8, 8])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 8, 512]), V=torch.Size([1, 8, 512]), mask=torch.Size([1, 1, 8, 8])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 8]); mask:torch.Size([1, 1, 8, 8])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 8, 512]), V=torch.Size([1, 8, 512]), mask=torch.Size([1, 1, 8, 8])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 8]); mask:torch.Size([1, 1, 8, 8])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 8, 512]), V=torch.Size([1, 8, 512]), mask=torch.Size([1, 1, 8, 8])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 8]); mask:torch.Size([1, 1, 8, 8])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 8, 512]), V=torch.Size([1, 8, 512]), mask=torch.Size([1, 1, 8, 8])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 8]); mask:torch.Size([1, 1, 8, 8])
in MultiheadAttention: Q=torch.Size([1, 8, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 8, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 9, 512]), V=torch.Size([1, 9, 512]), mask=torch.Size([1, 1, 9, 9])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 9]); mask:torch.Size([1, 1, 9, 9])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 9, 512]), V=torch.Size([1, 9, 512]), mask=torch.Size([1, 1, 9, 9])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 9]); mask:torch.Size([1, 1, 9, 9])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 9, 512]), V=torch.Size([1, 9, 512]), mask=torch.Size([1, 1, 9, 9])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 9]); mask:torch.Size([1, 1, 9, 9])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 9, 512]), V=torch.Size([1, 9, 512]), mask=torch.Size([1, 1, 9, 9])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 9]); mask:torch.Size([1, 1, 9, 9])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 9, 512]), V=torch.Size([1, 9, 512]), mask=torch.Size([1, 1, 9, 9])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 9]); mask:torch.Size([1, 1, 9, 9])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 9, 512]), V=torch.Size([1, 9, 512]), mask=torch.Size([1, 1, 9, 9])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 9]); mask:torch.Size([1, 1, 9, 9])
in MultiheadAttention: Q=torch.Size([1, 9, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 9, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 10, 512]), V=torch.Size([1, 10, 512]), mask=torch.Size([1, 1, 10, 10])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 10]); mask:torch.Size([1, 1, 10, 10])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 10, 512]), V=torch.Size([1, 10, 512]), mask=torch.Size([1, 1, 10, 10])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 10]); mask:torch.Size([1, 1, 10, 10])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 10, 512]), V=torch.Size([1, 10, 512]), mask=torch.Size([1, 1, 10, 10])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 10]); mask:torch.Size([1, 1, 10, 10])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 10, 512]), V=torch.Size([1, 10, 512]), mask=torch.Size([1, 1, 10, 10])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 10]); mask:torch.Size([1, 1, 10, 10])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 10, 512]), V=torch.Size([1, 10, 512]), mask=torch.Size([1, 1, 10, 10])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 10]); mask:torch.Size([1, 1, 10, 10])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 10, 512]), V=torch.Size([1, 10, 512]), mask=torch.Size([1, 1, 10, 10])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 10]); mask:torch.Size([1, 1, 10, 10])
in MultiheadAttention: Q=torch.Size([1, 10, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 10, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 11, 512]), V=torch.Size([1, 11, 512]), mask=torch.Size([1, 1, 11, 11])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 11]); mask:torch.Size([1, 1, 11, 11])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 11, 512]), V=torch.Size([1, 11, 512]), mask=torch.Size([1, 1, 11, 11])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 11]); mask:torch.Size([1, 1, 11, 11])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 11, 512]), V=torch.Size([1, 11, 512]), mask=torch.Size([1, 1, 11, 11])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 11]); mask:torch.Size([1, 1, 11, 11])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 11, 512]), V=torch.Size([1, 11, 512]), mask=torch.Size([1, 1, 11, 11])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 11]); mask:torch.Size([1, 1, 11, 11])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 11, 512]), V=torch.Size([1, 11, 512]), mask=torch.Size([1, 1, 11, 11])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 11]); mask:torch.Size([1, 1, 11, 11])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 11, 512]), V=torch.Size([1, 11, 512]), mask=torch.Size([1, 1, 11, 11])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 11]); mask:torch.Size([1, 1, 11, 11])
in MultiheadAttention: Q=torch.Size([1, 11, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 11, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 12, 512]), V=torch.Size([1, 12, 512]), mask=torch.Size([1, 1, 12, 12])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 12]); mask:torch.Size([1, 1, 12, 12])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 12, 512]), V=torch.Size([1, 12, 512]), mask=torch.Size([1, 1, 12, 12])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 12]); mask:torch.Size([1, 1, 12, 12])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 12, 512]), V=torch.Size([1, 12, 512]), mask=torch.Size([1, 1, 12, 12])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 12]); mask:torch.Size([1, 1, 12, 12])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 12, 512]), V=torch.Size([1, 12, 512]), mask=torch.Size([1, 1, 12, 12])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 12]); mask:torch.Size([1, 1, 12, 12])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 12, 512]), V=torch.Size([1, 12, 512]), mask=torch.Size([1, 1, 12, 12])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 12]); mask:torch.Size([1, 1, 12, 12])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 12, 512]), V=torch.Size([1, 12, 512]), mask=torch.Size([1, 1, 12, 12])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 12]); mask:torch.Size([1, 1, 12, 12])
in MultiheadAttention: Q=torch.Size([1, 12, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 12, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 13, 512]), V=torch.Size([1, 13, 512]), mask=torch.Size([1, 1, 13, 13])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 13]); mask:torch.Size([1, 1, 13, 13])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 13, 512]), V=torch.Size([1, 13, 512]), mask=torch.Size([1, 1, 13, 13])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 13]); mask:torch.Size([1, 1, 13, 13])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 13, 512]), V=torch.Size([1, 13, 512]), mask=torch.Size([1, 1, 13, 13])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 13]); mask:torch.Size([1, 1, 13, 13])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 13, 512]), V=torch.Size([1, 13, 512]), mask=torch.Size([1, 1, 13, 13])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 13]); mask:torch.Size([1, 1, 13, 13])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 13, 512]), V=torch.Size([1, 13, 512]), mask=torch.Size([1, 1, 13, 13])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 13]); mask:torch.Size([1, 1, 13, 13])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 13, 512]), V=torch.Size([1, 13, 512]), mask=torch.Size([1, 1, 13, 13])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 13]); mask:torch.Size([1, 1, 13, 13])
in MultiheadAttention: Q=torch.Size([1, 13, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 13, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 14, 512]), V=torch.Size([1, 14, 512]), mask=torch.Size([1, 1, 14, 14])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 14]); mask:torch.Size([1, 1, 14, 14])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 14, 512]), V=torch.Size([1, 14, 512]), mask=torch.Size([1, 1, 14, 14])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 14]); mask:torch.Size([1, 1, 14, 14])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 14, 512]), V=torch.Size([1, 14, 512]), mask=torch.Size([1, 1, 14, 14])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 14]); mask:torch.Size([1, 1, 14, 14])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 14, 512]), V=torch.Size([1, 14, 512]), mask=torch.Size([1, 1, 14, 14])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 14]); mask:torch.Size([1, 1, 14, 14])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 14, 512]), V=torch.Size([1, 14, 512]), mask=torch.Size([1, 1, 14, 14])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 14]); mask:torch.Size([1, 1, 14, 14])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 14, 512]), V=torch.Size([1, 14, 512]), mask=torch.Size([1, 1, 14, 14])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 14]); mask:torch.Size([1, 1, 14, 14])
in MultiheadAttention: Q=torch.Size([1, 14, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 14, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 15, 512]), V=torch.Size([1, 15, 512]), mask=torch.Size([1, 1, 15, 15])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 15]); mask:torch.Size([1, 1, 15, 15])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 15, 512]), V=torch.Size([1, 15, 512]), mask=torch.Size([1, 1, 15, 15])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 15]); mask:torch.Size([1, 1, 15, 15])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 15, 512]), V=torch.Size([1, 15, 512]), mask=torch.Size([1, 1, 15, 15])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 15]); mask:torch.Size([1, 1, 15, 15])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 15, 512]), V=torch.Size([1, 15, 512]), mask=torch.Size([1, 1, 15, 15])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 15]); mask:torch.Size([1, 1, 15, 15])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 15, 512]), V=torch.Size([1, 15, 512]), mask=torch.Size([1, 1, 15, 15])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 15]); mask:torch.Size([1, 1, 15, 15])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 15, 512]), V=torch.Size([1, 15, 512]), mask=torch.Size([1, 1, 15, 15])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 15]); mask:torch.Size([1, 1, 15, 15])
in MultiheadAttention: Q=torch.Size([1, 15, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 15, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 16, 512]), V=torch.Size([1, 16, 512]), mask=torch.Size([1, 1, 16, 16])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 16]); mask:torch.Size([1, 1, 16, 16])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 16, 512]), V=torch.Size([1, 16, 512]), mask=torch.Size([1, 1, 16, 16])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 16]); mask:torch.Size([1, 1, 16, 16])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 16, 512]), V=torch.Size([1, 16, 512]), mask=torch.Size([1, 1, 16, 16])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 16]); mask:torch.Size([1, 1, 16, 16])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 16, 512]), V=torch.Size([1, 16, 512]), mask=torch.Size([1, 1, 16, 16])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 16]); mask:torch.Size([1, 1, 16, 16])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 16, 512]), V=torch.Size([1, 16, 512]), mask=torch.Size([1, 1, 16, 16])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 16]); mask:torch.Size([1, 1, 16, 16])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 16, 512]), V=torch.Size([1, 16, 512]), mask=torch.Size([1, 1, 16, 16])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 16]); mask:torch.Size([1, 1, 16, 16])
in MultiheadAttention: Q=torch.Size([1, 16, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 16, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 17, 512]), V=torch.Size([1, 17, 512]), mask=torch.Size([1, 1, 17, 17])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 17]); mask:torch.Size([1, 1, 17, 17])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 17, 512]), V=torch.Size([1, 17, 512]), mask=torch.Size([1, 1, 17, 17])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 17]); mask:torch.Size([1, 1, 17, 17])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 17, 512]), V=torch.Size([1, 17, 512]), mask=torch.Size([1, 1, 17, 17])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 17]); mask:torch.Size([1, 1, 17, 17])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 17, 512]), V=torch.Size([1, 17, 512]), mask=torch.Size([1, 1, 17, 17])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 17]); mask:torch.Size([1, 1, 17, 17])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 17, 512]), V=torch.Size([1, 17, 512]), mask=torch.Size([1, 1, 17, 17])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 17]); mask:torch.Size([1, 1, 17, 17])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 17, 512]), V=torch.Size([1, 17, 512]), mask=torch.Size([1, 1, 17, 17])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 17]); mask:torch.Size([1, 1, 17, 17])
in MultiheadAttention: Q=torch.Size([1, 17, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 17, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 18, 512]), V=torch.Size([1, 18, 512]), mask=torch.Size([1, 1, 18, 18])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 18]); mask:torch.Size([1, 1, 18, 18])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 18, 512]), V=torch.Size([1, 18, 512]), mask=torch.Size([1, 1, 18, 18])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 18]); mask:torch.Size([1, 1, 18, 18])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 18, 512]), V=torch.Size([1, 18, 512]), mask=torch.Size([1, 1, 18, 18])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 18]); mask:torch.Size([1, 1, 18, 18])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 18, 512]), V=torch.Size([1, 18, 512]), mask=torch.Size([1, 1, 18, 18])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 18]); mask:torch.Size([1, 1, 18, 18])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 18, 512]), V=torch.Size([1, 18, 512]), mask=torch.Size([1, 1, 18, 18])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 18]); mask:torch.Size([1, 1, 18, 18])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 18, 512]), V=torch.Size([1, 18, 512]), mask=torch.Size([1, 1, 18, 18])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 18]); mask:torch.Size([1, 1, 18, 18])
in MultiheadAttention: Q=torch.Size([1, 18, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 18, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 19, 512]), V=torch.Size([1, 19, 512]), mask=torch.Size([1, 1, 19, 19])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 19]); mask:torch.Size([1, 1, 19, 19])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 19, 512]), V=torch.Size([1, 19, 512]), mask=torch.Size([1, 1, 19, 19])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 19]); mask:torch.Size([1, 1, 19, 19])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 19, 512]), V=torch.Size([1, 19, 512]), mask=torch.Size([1, 1, 19, 19])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 19]); mask:torch.Size([1, 1, 19, 19])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 19, 512]), V=torch.Size([1, 19, 512]), mask=torch.Size([1, 1, 19, 19])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 19]); mask:torch.Size([1, 1, 19, 19])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 19, 512]), V=torch.Size([1, 19, 512]), mask=torch.Size([1, 1, 19, 19])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 19]); mask:torch.Size([1, 1, 19, 19])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 19, 512]), V=torch.Size([1, 19, 512]), mask=torch.Size([1, 1, 19, 19])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 19]); mask:torch.Size([1, 1, 19, 19])
in MultiheadAttention: Q=torch.Size([1, 19, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 19, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 20, 512]), V=torch.Size([1, 20, 512]), mask=torch.Size([1, 1, 20, 20])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 20]); mask:torch.Size([1, 1, 20, 20])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 20, 512]), V=torch.Size([1, 20, 512]), mask=torch.Size([1, 1, 20, 20])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 20]); mask:torch.Size([1, 1, 20, 20])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 20, 512]), V=torch.Size([1, 20, 512]), mask=torch.Size([1, 1, 20, 20])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 20]); mask:torch.Size([1, 1, 20, 20])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 20, 512]), V=torch.Size([1, 20, 512]), mask=torch.Size([1, 1, 20, 20])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 20]); mask:torch.Size([1, 1, 20, 20])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 20, 512]), V=torch.Size([1, 20, 512]), mask=torch.Size([1, 1, 20, 20])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 20]); mask:torch.Size([1, 1, 20, 20])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 20, 512]), V=torch.Size([1, 20, 512]), mask=torch.Size([1, 1, 20, 20])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 20]); mask:torch.Size([1, 1, 20, 20])
in MultiheadAttention: Q=torch.Size([1, 20, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 20, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 21, 512]), V=torch.Size([1, 21, 512]), mask=torch.Size([1, 1, 21, 21])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 21]); mask:torch.Size([1, 1, 21, 21])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 21, 512]), V=torch.Size([1, 21, 512]), mask=torch.Size([1, 1, 21, 21])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 21]); mask:torch.Size([1, 1, 21, 21])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 21, 512]), V=torch.Size([1, 21, 512]), mask=torch.Size([1, 1, 21, 21])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 21]); mask:torch.Size([1, 1, 21, 21])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 21, 512]), V=torch.Size([1, 21, 512]), mask=torch.Size([1, 1, 21, 21])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 21]); mask:torch.Size([1, 1, 21, 21])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 21, 512]), V=torch.Size([1, 21, 512]), mask=torch.Size([1, 1, 21, 21])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 21]); mask:torch.Size([1, 1, 21, 21])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 21, 512]), V=torch.Size([1, 21, 512]), mask=torch.Size([1, 1, 21, 21])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 21]); mask:torch.Size([1, 1, 21, 21])
in MultiheadAttention: Q=torch.Size([1, 21, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 21, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 22, 512]), V=torch.Size([1, 22, 512]), mask=torch.Size([1, 1, 22, 22])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 22]); mask:torch.Size([1, 1, 22, 22])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 22, 512]), V=torch.Size([1, 22, 512]), mask=torch.Size([1, 1, 22, 22])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 22]); mask:torch.Size([1, 1, 22, 22])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 22, 512]), V=torch.Size([1, 22, 512]), mask=torch.Size([1, 1, 22, 22])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 22]); mask:torch.Size([1, 1, 22, 22])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 22, 512]), V=torch.Size([1, 22, 512]), mask=torch.Size([1, 1, 22, 22])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 22]); mask:torch.Size([1, 1, 22, 22])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 22, 512]), V=torch.Size([1, 22, 512]), mask=torch.Size([1, 1, 22, 22])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 22]); mask:torch.Size([1, 1, 22, 22])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 22, 512]), V=torch.Size([1, 22, 512]), mask=torch.Size([1, 1, 22, 22])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 22]); mask:torch.Size([1, 1, 22, 22])
in MultiheadAttention: Q=torch.Size([1, 22, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 22, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 23, 512]), V=torch.Size([1, 23, 512]), mask=torch.Size([1, 1, 23, 23])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 23]); mask:torch.Size([1, 1, 23, 23])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 23, 512]), V=torch.Size([1, 23, 512]), mask=torch.Size([1, 1, 23, 23])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 23]); mask:torch.Size([1, 1, 23, 23])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 23, 512]), V=torch.Size([1, 23, 512]), mask=torch.Size([1, 1, 23, 23])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 23]); mask:torch.Size([1, 1, 23, 23])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 23, 512]), V=torch.Size([1, 23, 512]), mask=torch.Size([1, 1, 23, 23])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 23]); mask:torch.Size([1, 1, 23, 23])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 23, 512]), V=torch.Size([1, 23, 512]), mask=torch.Size([1, 1, 23, 23])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 23]); mask:torch.Size([1, 1, 23, 23])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 23, 512]), V=torch.Size([1, 23, 512]), mask=torch.Size([1, 1, 23, 23])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 23]); mask:torch.Size([1, 1, 23, 23])
in MultiheadAttention: Q=torch.Size([1, 23, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 23, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 24, 512]), V=torch.Size([1, 24, 512]), mask=torch.Size([1, 1, 24, 24])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 24]); mask:torch.Size([1, 1, 24, 24])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 24, 512]), V=torch.Size([1, 24, 512]), mask=torch.Size([1, 1, 24, 24])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 24]); mask:torch.Size([1, 1, 24, 24])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 24, 512]), V=torch.Size([1, 24, 512]), mask=torch.Size([1, 1, 24, 24])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 24]); mask:torch.Size([1, 1, 24, 24])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 24, 512]), V=torch.Size([1, 24, 512]), mask=torch.Size([1, 1, 24, 24])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 24]); mask:torch.Size([1, 1, 24, 24])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 24, 512]), V=torch.Size([1, 24, 512]), mask=torch.Size([1, 1, 24, 24])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 24]); mask:torch.Size([1, 1, 24, 24])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 24, 512]), V=torch.Size([1, 24, 512]), mask=torch.Size([1, 1, 24, 24])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 24]); mask:torch.Size([1, 1, 24, 24])
in MultiheadAttention: Q=torch.Size([1, 24, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 24, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 25, 512]), V=torch.Size([1, 25, 512]), mask=torch.Size([1, 1, 25, 25])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 25]); mask:torch.Size([1, 1, 25, 25])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 25, 512]), V=torch.Size([1, 25, 512]), mask=torch.Size([1, 1, 25, 25])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 25]); mask:torch.Size([1, 1, 25, 25])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 25, 512]), V=torch.Size([1, 25, 512]), mask=torch.Size([1, 1, 25, 25])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 25]); mask:torch.Size([1, 1, 25, 25])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 25, 512]), V=torch.Size([1, 25, 512]), mask=torch.Size([1, 1, 25, 25])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 25]); mask:torch.Size([1, 1, 25, 25])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 25, 512]), V=torch.Size([1, 25, 512]), mask=torch.Size([1, 1, 25, 25])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 25]); mask:torch.Size([1, 1, 25, 25])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 25, 512]), V=torch.Size([1, 25, 512]), mask=torch.Size([1, 1, 25, 25])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 25]); mask:torch.Size([1, 1, 25, 25])
in MultiheadAttention: Q=torch.Size([1, 25, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 25, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 26, 512]), V=torch.Size([1, 26, 512]), mask=torch.Size([1, 1, 26, 26])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 26]); mask:torch.Size([1, 1, 26, 26])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 26, 512]), V=torch.Size([1, 26, 512]), mask=torch.Size([1, 1, 26, 26])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 26]); mask:torch.Size([1, 1, 26, 26])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 26, 512]), V=torch.Size([1, 26, 512]), mask=torch.Size([1, 1, 26, 26])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 26]); mask:torch.Size([1, 1, 26, 26])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 26, 512]), V=torch.Size([1, 26, 512]), mask=torch.Size([1, 1, 26, 26])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 26]); mask:torch.Size([1, 1, 26, 26])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 26, 512]), V=torch.Size([1, 26, 512]), mask=torch.Size([1, 1, 26, 26])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 26]); mask:torch.Size([1, 1, 26, 26])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 26, 512]), V=torch.Size([1, 26, 512]), mask=torch.Size([1, 1, 26, 26])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 26]); mask:torch.Size([1, 1, 26, 26])
in MultiheadAttention: Q=torch.Size([1, 26, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 26, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 27, 512]), V=torch.Size([1, 27, 512]), mask=torch.Size([1, 1, 27, 27])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 27]); mask:torch.Size([1, 1, 27, 27])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 27, 512]), V=torch.Size([1, 27, 512]), mask=torch.Size([1, 1, 27, 27])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 27]); mask:torch.Size([1, 1, 27, 27])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 27, 512]), V=torch.Size([1, 27, 512]), mask=torch.Size([1, 1, 27, 27])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 27]); mask:torch.Size([1, 1, 27, 27])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 27, 512]), V=torch.Size([1, 27, 512]), mask=torch.Size([1, 1, 27, 27])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 27]); mask:torch.Size([1, 1, 27, 27])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 27, 512]), V=torch.Size([1, 27, 512]), mask=torch.Size([1, 1, 27, 27])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 27]); mask:torch.Size([1, 1, 27, 27])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 27, 512]), V=torch.Size([1, 27, 512]), mask=torch.Size([1, 1, 27, 27])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 27]); mask:torch.Size([1, 1, 27, 27])
in MultiheadAttention: Q=torch.Size([1, 27, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 27, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 28, 512]), V=torch.Size([1, 28, 512]), mask=torch.Size([1, 1, 28, 28])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 28]); mask:torch.Size([1, 1, 28, 28])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 28, 512]), V=torch.Size([1, 28, 512]), mask=torch.Size([1, 1, 28, 28])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 28]); mask:torch.Size([1, 1, 28, 28])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 28, 512]), V=torch.Size([1, 28, 512]), mask=torch.Size([1, 1, 28, 28])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 28]); mask:torch.Size([1, 1, 28, 28])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 28, 512]), V=torch.Size([1, 28, 512]), mask=torch.Size([1, 1, 28, 28])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 28]); mask:torch.Size([1, 1, 28, 28])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 28, 512]), V=torch.Size([1, 28, 512]), mask=torch.Size([1, 1, 28, 28])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 28]); mask:torch.Size([1, 1, 28, 28])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 28, 512]), V=torch.Size([1, 28, 512]), mask=torch.Size([1, 1, 28, 28])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 28]); mask:torch.Size([1, 1, 28, 28])
in MultiheadAttention: Q=torch.Size([1, 28, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 28, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 29, 512]), V=torch.Size([1, 29, 512]), mask=torch.Size([1, 1, 29, 29])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 29]); mask:torch.Size([1, 1, 29, 29])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 29, 512]), V=torch.Size([1, 29, 512]), mask=torch.Size([1, 1, 29, 29])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 29]); mask:torch.Size([1, 1, 29, 29])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 29, 512]), V=torch.Size([1, 29, 512]), mask=torch.Size([1, 1, 29, 29])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 29]); mask:torch.Size([1, 1, 29, 29])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 29, 512]), V=torch.Size([1, 29, 512]), mask=torch.Size([1, 1, 29, 29])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 29]); mask:torch.Size([1, 1, 29, 29])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 29, 512]), V=torch.Size([1, 29, 512]), mask=torch.Size([1, 1, 29, 29])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 29]); mask:torch.Size([1, 1, 29, 29])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 29, 512]), V=torch.Size([1, 29, 512]), mask=torch.Size([1, 1, 29, 29])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 29]); mask:torch.Size([1, 1, 29, 29])
in MultiheadAttention: Q=torch.Size([1, 29, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 29, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 30, 512]), V=torch.Size([1, 30, 512]), mask=torch.Size([1, 1, 30, 30])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 30]); mask:torch.Size([1, 1, 30, 30])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 30, 512]), V=torch.Size([1, 30, 512]), mask=torch.Size([1, 1, 30, 30])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 30]); mask:torch.Size([1, 1, 30, 30])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 30, 512]), V=torch.Size([1, 30, 512]), mask=torch.Size([1, 1, 30, 30])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 30]); mask:torch.Size([1, 1, 30, 30])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 30, 512]), V=torch.Size([1, 30, 512]), mask=torch.Size([1, 1, 30, 30])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 30]); mask:torch.Size([1, 1, 30, 30])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 30, 512]), V=torch.Size([1, 30, 512]), mask=torch.Size([1, 1, 30, 30])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 30]); mask:torch.Size([1, 1, 30, 30])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 30, 512]), V=torch.Size([1, 30, 512]), mask=torch.Size([1, 1, 30, 30])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 30]); mask:torch.Size([1, 1, 30, 30])
in MultiheadAttention: Q=torch.Size([1, 30, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 30, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 31, 512]), V=torch.Size([1, 31, 512]), mask=torch.Size([1, 1, 31, 31])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 31]); mask:torch.Size([1, 1, 31, 31])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 31, 512]), V=torch.Size([1, 31, 512]), mask=torch.Size([1, 1, 31, 31])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 31]); mask:torch.Size([1, 1, 31, 31])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 31, 512]), V=torch.Size([1, 31, 512]), mask=torch.Size([1, 1, 31, 31])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 31]); mask:torch.Size([1, 1, 31, 31])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 31, 512]), V=torch.Size([1, 31, 512]), mask=torch.Size([1, 1, 31, 31])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 31]); mask:torch.Size([1, 1, 31, 31])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 31, 512]), V=torch.Size([1, 31, 512]), mask=torch.Size([1, 1, 31, 31])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 31]); mask:torch.Size([1, 1, 31, 31])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 31, 512]), V=torch.Size([1, 31, 512]), mask=torch.Size([1, 1, 31, 31])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 31]); mask:torch.Size([1, 1, 31, 31])
in MultiheadAttention: Q=torch.Size([1, 31, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 31, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 32, 512]), V=torch.Size([1, 32, 512]), mask=torch.Size([1, 1, 32, 32])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 32]); mask:torch.Size([1, 1, 32, 32])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 32, 512]), V=torch.Size([1, 32, 512]), mask=torch.Size([1, 1, 32, 32])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 32]); mask:torch.Size([1, 1, 32, 32])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 32, 512]), V=torch.Size([1, 32, 512]), mask=torch.Size([1, 1, 32, 32])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 32]); mask:torch.Size([1, 1, 32, 32])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 32, 512]), V=torch.Size([1, 32, 512]), mask=torch.Size([1, 1, 32, 32])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 32]); mask:torch.Size([1, 1, 32, 32])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 32, 512]), V=torch.Size([1, 32, 512]), mask=torch.Size([1, 1, 32, 32])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 32]); mask:torch.Size([1, 1, 32, 32])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 32, 512]), V=torch.Size([1, 32, 512]), mask=torch.Size([1, 1, 32, 32])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 32]); mask:torch.Size([1, 1, 32, 32])
in MultiheadAttention: Q=torch.Size([1, 32, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 32, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 33, 512]), V=torch.Size([1, 33, 512]), mask=torch.Size([1, 1, 33, 33])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 33]); mask:torch.Size([1, 1, 33, 33])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 33, 512]), V=torch.Size([1, 33, 512]), mask=torch.Size([1, 1, 33, 33])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 33]); mask:torch.Size([1, 1, 33, 33])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 33, 512]), V=torch.Size([1, 33, 512]), mask=torch.Size([1, 1, 33, 33])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 33]); mask:torch.Size([1, 1, 33, 33])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 33, 512]), V=torch.Size([1, 33, 512]), mask=torch.Size([1, 1, 33, 33])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 33]); mask:torch.Size([1, 1, 33, 33])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 33, 512]), V=torch.Size([1, 33, 512]), mask=torch.Size([1, 1, 33, 33])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 33]); mask:torch.Size([1, 1, 33, 33])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 33, 512]), V=torch.Size([1, 33, 512]), mask=torch.Size([1, 1, 33, 33])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 33]); mask:torch.Size([1, 1, 33, 33])
in MultiheadAttention: Q=torch.Size([1, 33, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 33, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 34, 512]), V=torch.Size([1, 34, 512]), mask=torch.Size([1, 1, 34, 34])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 34]); mask:torch.Size([1, 1, 34, 34])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 34, 512]), V=torch.Size([1, 34, 512]), mask=torch.Size([1, 1, 34, 34])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 34]); mask:torch.Size([1, 1, 34, 34])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 34, 512]), V=torch.Size([1, 34, 512]), mask=torch.Size([1, 1, 34, 34])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 34]); mask:torch.Size([1, 1, 34, 34])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 34, 512]), V=torch.Size([1, 34, 512]), mask=torch.Size([1, 1, 34, 34])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 34]); mask:torch.Size([1, 1, 34, 34])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 34, 512]), V=torch.Size([1, 34, 512]), mask=torch.Size([1, 1, 34, 34])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 34]); mask:torch.Size([1, 1, 34, 34])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 34, 512]), V=torch.Size([1, 34, 512]), mask=torch.Size([1, 1, 34, 34])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 34]); mask:torch.Size([1, 1, 34, 34])
in MultiheadAttention: Q=torch.Size([1, 34, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 34, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 35, 512]), V=torch.Size([1, 35, 512]), mask=torch.Size([1, 1, 35, 35])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 35]); mask:torch.Size([1, 1, 35, 35])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 35, 512]), V=torch.Size([1, 35, 512]), mask=torch.Size([1, 1, 35, 35])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 35]); mask:torch.Size([1, 1, 35, 35])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 35, 512]), V=torch.Size([1, 35, 512]), mask=torch.Size([1, 1, 35, 35])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 35]); mask:torch.Size([1, 1, 35, 35])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 35, 512]), V=torch.Size([1, 35, 512]), mask=torch.Size([1, 1, 35, 35])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 35]); mask:torch.Size([1, 1, 35, 35])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 35, 512]), V=torch.Size([1, 35, 512]), mask=torch.Size([1, 1, 35, 35])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 35]); mask:torch.Size([1, 1, 35, 35])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 35, 512]), V=torch.Size([1, 35, 512]), mask=torch.Size([1, 1, 35, 35])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 35]); mask:torch.Size([1, 1, 35, 35])
in MultiheadAttention: Q=torch.Size([1, 35, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 35, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 36, 512]), V=torch.Size([1, 36, 512]), mask=torch.Size([1, 1, 36, 36])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 36]); mask:torch.Size([1, 1, 36, 36])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 36, 512]), V=torch.Size([1, 36, 512]), mask=torch.Size([1, 1, 36, 36])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 36]); mask:torch.Size([1, 1, 36, 36])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 36, 512]), V=torch.Size([1, 36, 512]), mask=torch.Size([1, 1, 36, 36])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 36]); mask:torch.Size([1, 1, 36, 36])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 36, 512]), V=torch.Size([1, 36, 512]), mask=torch.Size([1, 1, 36, 36])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 36]); mask:torch.Size([1, 1, 36, 36])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 36, 512]), V=torch.Size([1, 36, 512]), mask=torch.Size([1, 1, 36, 36])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 36]); mask:torch.Size([1, 1, 36, 36])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 36, 512]), V=torch.Size([1, 36, 512]), mask=torch.Size([1, 1, 36, 36])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 36]); mask:torch.Size([1, 1, 36, 36])
in MultiheadAttention: Q=torch.Size([1, 36, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 36, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 37, 512]), V=torch.Size([1, 37, 512]), mask=torch.Size([1, 1, 37, 37])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 37]); mask:torch.Size([1, 1, 37, 37])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 37, 512]), V=torch.Size([1, 37, 512]), mask=torch.Size([1, 1, 37, 37])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 37]); mask:torch.Size([1, 1, 37, 37])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 37, 512]), V=torch.Size([1, 37, 512]), mask=torch.Size([1, 1, 37, 37])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 37]); mask:torch.Size([1, 1, 37, 37])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 37, 512]), V=torch.Size([1, 37, 512]), mask=torch.Size([1, 1, 37, 37])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 37]); mask:torch.Size([1, 1, 37, 37])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 37, 512]), V=torch.Size([1, 37, 512]), mask=torch.Size([1, 1, 37, 37])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 37]); mask:torch.Size([1, 1, 37, 37])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 37, 512]), V=torch.Size([1, 37, 512]), mask=torch.Size([1, 1, 37, 37])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 37]); mask:torch.Size([1, 1, 37, 37])
in MultiheadAttention: Q=torch.Size([1, 37, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 37, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 38, 512]), V=torch.Size([1, 38, 512]), mask=torch.Size([1, 1, 38, 38])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 38]); mask:torch.Size([1, 1, 38, 38])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 38, 512]), V=torch.Size([1, 38, 512]), mask=torch.Size([1, 1, 38, 38])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 38]); mask:torch.Size([1, 1, 38, 38])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 38, 512]), V=torch.Size([1, 38, 512]), mask=torch.Size([1, 1, 38, 38])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 38]); mask:torch.Size([1, 1, 38, 38])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 38, 512]), V=torch.Size([1, 38, 512]), mask=torch.Size([1, 1, 38, 38])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 38]); mask:torch.Size([1, 1, 38, 38])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 38, 512]), V=torch.Size([1, 38, 512]), mask=torch.Size([1, 1, 38, 38])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 38]); mask:torch.Size([1, 1, 38, 38])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 38, 512]), V=torch.Size([1, 38, 512]), mask=torch.Size([1, 1, 38, 38])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 38]); mask:torch.Size([1, 1, 38, 38])
in MultiheadAttention: Q=torch.Size([1, 38, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 38, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 39, 512]), V=torch.Size([1, 39, 512]), mask=torch.Size([1, 1, 39, 39])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 39]); mask:torch.Size([1, 1, 39, 39])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 39, 512]), V=torch.Size([1, 39, 512]), mask=torch.Size([1, 1, 39, 39])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 39]); mask:torch.Size([1, 1, 39, 39])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 39, 512]), V=torch.Size([1, 39, 512]), mask=torch.Size([1, 1, 39, 39])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 39]); mask:torch.Size([1, 1, 39, 39])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 39, 512]), V=torch.Size([1, 39, 512]), mask=torch.Size([1, 1, 39, 39])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 39]); mask:torch.Size([1, 1, 39, 39])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 39, 512]), V=torch.Size([1, 39, 512]), mask=torch.Size([1, 1, 39, 39])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 39]); mask:torch.Size([1, 1, 39, 39])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 39, 512]), V=torch.Size([1, 39, 512]), mask=torch.Size([1, 1, 39, 39])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 39]); mask:torch.Size([1, 1, 39, 39])
in MultiheadAttention: Q=torch.Size([1, 39, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 39, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 40, 512]), V=torch.Size([1, 40, 512]), mask=torch.Size([1, 1, 40, 40])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 40]); mask:torch.Size([1, 1, 40, 40])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 40, 512]), V=torch.Size([1, 40, 512]), mask=torch.Size([1, 1, 40, 40])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 40]); mask:torch.Size([1, 1, 40, 40])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 40, 512]), V=torch.Size([1, 40, 512]), mask=torch.Size([1, 1, 40, 40])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 40]); mask:torch.Size([1, 1, 40, 40])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 40, 512]), V=torch.Size([1, 40, 512]), mask=torch.Size([1, 1, 40, 40])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 40]); mask:torch.Size([1, 1, 40, 40])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 40, 512]), V=torch.Size([1, 40, 512]), mask=torch.Size([1, 1, 40, 40])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 40]); mask:torch.Size([1, 1, 40, 40])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 40, 512]), V=torch.Size([1, 40, 512]), mask=torch.Size([1, 1, 40, 40])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 40]); mask:torch.Size([1, 1, 40, 40])
in MultiheadAttention: Q=torch.Size([1, 40, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 40, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 41, 512]), V=torch.Size([1, 41, 512]), mask=torch.Size([1, 1, 41, 41])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 41]); mask:torch.Size([1, 1, 41, 41])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 41, 512]), V=torch.Size([1, 41, 512]), mask=torch.Size([1, 1, 41, 41])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 41]); mask:torch.Size([1, 1, 41, 41])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 41, 512]), V=torch.Size([1, 41, 512]), mask=torch.Size([1, 1, 41, 41])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 41]); mask:torch.Size([1, 1, 41, 41])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 41, 512]), V=torch.Size([1, 41, 512]), mask=torch.Size([1, 1, 41, 41])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 41]); mask:torch.Size([1, 1, 41, 41])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 41, 512]), V=torch.Size([1, 41, 512]), mask=torch.Size([1, 1, 41, 41])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 41]); mask:torch.Size([1, 1, 41, 41])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 41, 512]), V=torch.Size([1, 41, 512]), mask=torch.Size([1, 1, 41, 41])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 41]); mask:torch.Size([1, 1, 41, 41])
in MultiheadAttention: Q=torch.Size([1, 41, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 41, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 42, 512]), V=torch.Size([1, 42, 512]), mask=torch.Size([1, 1, 42, 42])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 42]); mask:torch.Size([1, 1, 42, 42])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 42, 512]), V=torch.Size([1, 42, 512]), mask=torch.Size([1, 1, 42, 42])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 42]); mask:torch.Size([1, 1, 42, 42])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 42, 512]), V=torch.Size([1, 42, 512]), mask=torch.Size([1, 1, 42, 42])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 42]); mask:torch.Size([1, 1, 42, 42])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 42, 512]), V=torch.Size([1, 42, 512]), mask=torch.Size([1, 1, 42, 42])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 42]); mask:torch.Size([1, 1, 42, 42])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 42, 512]), V=torch.Size([1, 42, 512]), mask=torch.Size([1, 1, 42, 42])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 42]); mask:torch.Size([1, 1, 42, 42])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 42, 512]), V=torch.Size([1, 42, 512]), mask=torch.Size([1, 1, 42, 42])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 42]); mask:torch.Size([1, 1, 42, 42])
in MultiheadAttention: Q=torch.Size([1, 42, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 42, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 43, 512]), V=torch.Size([1, 43, 512]), mask=torch.Size([1, 1, 43, 43])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 43]); mask:torch.Size([1, 1, 43, 43])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 43, 512]), V=torch.Size([1, 43, 512]), mask=torch.Size([1, 1, 43, 43])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 43]); mask:torch.Size([1, 1, 43, 43])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 43, 512]), V=torch.Size([1, 43, 512]), mask=torch.Size([1, 1, 43, 43])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 43]); mask:torch.Size([1, 1, 43, 43])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 43, 512]), V=torch.Size([1, 43, 512]), mask=torch.Size([1, 1, 43, 43])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 43]); mask:torch.Size([1, 1, 43, 43])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 43, 512]), V=torch.Size([1, 43, 512]), mask=torch.Size([1, 1, 43, 43])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 43]); mask:torch.Size([1, 1, 43, 43])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 43, 512]), V=torch.Size([1, 43, 512]), mask=torch.Size([1, 1, 43, 43])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 43]); mask:torch.Size([1, 1, 43, 43])
in MultiheadAttention: Q=torch.Size([1, 43, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 43, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 44, 512]), V=torch.Size([1, 44, 512]), mask=torch.Size([1, 1, 44, 44])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 44]); mask:torch.Size([1, 1, 44, 44])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 44, 512]), V=torch.Size([1, 44, 512]), mask=torch.Size([1, 1, 44, 44])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 44]); mask:torch.Size([1, 1, 44, 44])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 44, 512]), V=torch.Size([1, 44, 512]), mask=torch.Size([1, 1, 44, 44])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 44]); mask:torch.Size([1, 1, 44, 44])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 44, 512]), V=torch.Size([1, 44, 512]), mask=torch.Size([1, 1, 44, 44])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 44]); mask:torch.Size([1, 1, 44, 44])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 44, 512]), V=torch.Size([1, 44, 512]), mask=torch.Size([1, 1, 44, 44])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 44]); mask:torch.Size([1, 1, 44, 44])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 44, 512]), V=torch.Size([1, 44, 512]), mask=torch.Size([1, 1, 44, 44])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 44]); mask:torch.Size([1, 1, 44, 44])
in MultiheadAttention: Q=torch.Size([1, 44, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 44, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 45, 512]), V=torch.Size([1, 45, 512]), mask=torch.Size([1, 1, 45, 45])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 45]); mask:torch.Size([1, 1, 45, 45])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 45, 512]), V=torch.Size([1, 45, 512]), mask=torch.Size([1, 1, 45, 45])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 45]); mask:torch.Size([1, 1, 45, 45])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 45, 512]), V=torch.Size([1, 45, 512]), mask=torch.Size([1, 1, 45, 45])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 45]); mask:torch.Size([1, 1, 45, 45])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 45, 512]), V=torch.Size([1, 45, 512]), mask=torch.Size([1, 1, 45, 45])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 45]); mask:torch.Size([1, 1, 45, 45])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 45, 512]), V=torch.Size([1, 45, 512]), mask=torch.Size([1, 1, 45, 45])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 45]); mask:torch.Size([1, 1, 45, 45])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 45, 512]), V=torch.Size([1, 45, 512]), mask=torch.Size([1, 1, 45, 45])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 45]); mask:torch.Size([1, 1, 45, 45])
in MultiheadAttention: Q=torch.Size([1, 45, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 45, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 46, 512]), V=torch.Size([1, 46, 512]), mask=torch.Size([1, 1, 46, 46])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 46]); mask:torch.Size([1, 1, 46, 46])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 46, 512]), V=torch.Size([1, 46, 512]), mask=torch.Size([1, 1, 46, 46])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 46]); mask:torch.Size([1, 1, 46, 46])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 46, 512]), V=torch.Size([1, 46, 512]), mask=torch.Size([1, 1, 46, 46])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 46]); mask:torch.Size([1, 1, 46, 46])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 46, 512]), V=torch.Size([1, 46, 512]), mask=torch.Size([1, 1, 46, 46])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 46]); mask:torch.Size([1, 1, 46, 46])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 46, 512]), V=torch.Size([1, 46, 512]), mask=torch.Size([1, 1, 46, 46])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 46]); mask:torch.Size([1, 1, 46, 46])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 46, 512]), V=torch.Size([1, 46, 512]), mask=torch.Size([1, 1, 46, 46])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 46]); mask:torch.Size([1, 1, 46, 46])
in MultiheadAttention: Q=torch.Size([1, 46, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 46, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 47, 512]), V=torch.Size([1, 47, 512]), mask=torch.Size([1, 1, 47, 47])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 47]); mask:torch.Size([1, 1, 47, 47])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 47, 512]), V=torch.Size([1, 47, 512]), mask=torch.Size([1, 1, 47, 47])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 47]); mask:torch.Size([1, 1, 47, 47])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 47, 512]), V=torch.Size([1, 47, 512]), mask=torch.Size([1, 1, 47, 47])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 47]); mask:torch.Size([1, 1, 47, 47])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 47, 512]), V=torch.Size([1, 47, 512]), mask=torch.Size([1, 1, 47, 47])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 47]); mask:torch.Size([1, 1, 47, 47])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 47, 512]), V=torch.Size([1, 47, 512]), mask=torch.Size([1, 1, 47, 47])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 47]); mask:torch.Size([1, 1, 47, 47])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 47, 512]), V=torch.Size([1, 47, 512]), mask=torch.Size([1, 1, 47, 47])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 47]); mask:torch.Size([1, 1, 47, 47])
in MultiheadAttention: Q=torch.Size([1, 47, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 47, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 48, 512]), V=torch.Size([1, 48, 512]), mask=torch.Size([1, 1, 48, 48])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 48]); mask:torch.Size([1, 1, 48, 48])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 48, 512]), V=torch.Size([1, 48, 512]), mask=torch.Size([1, 1, 48, 48])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 48]); mask:torch.Size([1, 1, 48, 48])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 48, 512]), V=torch.Size([1, 48, 512]), mask=torch.Size([1, 1, 48, 48])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 48]); mask:torch.Size([1, 1, 48, 48])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 48, 512]), V=torch.Size([1, 48, 512]), mask=torch.Size([1, 1, 48, 48])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 48]); mask:torch.Size([1, 1, 48, 48])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 48, 512]), V=torch.Size([1, 48, 512]), mask=torch.Size([1, 1, 48, 48])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 48]); mask:torch.Size([1, 1, 48, 48])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 48, 512]), V=torch.Size([1, 48, 512]), mask=torch.Size([1, 1, 48, 48])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 48]); mask:torch.Size([1, 1, 48, 48])
in MultiheadAttention: Q=torch.Size([1, 48, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 48, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 49, 512]), V=torch.Size([1, 49, 512]), mask=torch.Size([1, 1, 49, 49])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 49]); mask:torch.Size([1, 1, 49, 49])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 49, 512]), V=torch.Size([1, 49, 512]), mask=torch.Size([1, 1, 49, 49])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 49]); mask:torch.Size([1, 1, 49, 49])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 49, 512]), V=torch.Size([1, 49, 512]), mask=torch.Size([1, 1, 49, 49])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 49]); mask:torch.Size([1, 1, 49, 49])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 49, 512]), V=torch.Size([1, 49, 512]), mask=torch.Size([1, 1, 49, 49])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 49]); mask:torch.Size([1, 1, 49, 49])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 49, 512]), V=torch.Size([1, 49, 512]), mask=torch.Size([1, 1, 49, 49])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 49]); mask:torch.Size([1, 1, 49, 49])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 49, 512]), V=torch.Size([1, 49, 512]), mask=torch.Size([1, 1, 49, 49])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 49]); mask:torch.Size([1, 1, 49, 49])
in MultiheadAttention: Q=torch.Size([1, 49, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 49, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 50, 512]), V=torch.Size([1, 50, 512]), mask=torch.Size([1, 1, 50, 50])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 50]); mask:torch.Size([1, 1, 50, 50])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 50, 512]), V=torch.Size([1, 50, 512]), mask=torch.Size([1, 1, 50, 50])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 50]); mask:torch.Size([1, 1, 50, 50])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 50, 512]), V=torch.Size([1, 50, 512]), mask=torch.Size([1, 1, 50, 50])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 50]); mask:torch.Size([1, 1, 50, 50])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 50, 512]), V=torch.Size([1, 50, 512]), mask=torch.Size([1, 1, 50, 50])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 50]); mask:torch.Size([1, 1, 50, 50])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 50, 512]), V=torch.Size([1, 50, 512]), mask=torch.Size([1, 1, 50, 50])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 50]); mask:torch.Size([1, 1, 50, 50])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 50, 512]), V=torch.Size([1, 50, 512]), mask=torch.Size([1, 1, 50, 50])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 50]); mask:torch.Size([1, 1, 50, 50])
in MultiheadAttention: Q=torch.Size([1, 50, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 50, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 51, 512]), V=torch.Size([1, 51, 512]), mask=torch.Size([1, 1, 51, 51])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 51]); mask:torch.Size([1, 1, 51, 51])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 51, 512]), V=torch.Size([1, 51, 512]), mask=torch.Size([1, 1, 51, 51])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 51]); mask:torch.Size([1, 1, 51, 51])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 51, 512]), V=torch.Size([1, 51, 512]), mask=torch.Size([1, 1, 51, 51])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 51]); mask:torch.Size([1, 1, 51, 51])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 51, 512]), V=torch.Size([1, 51, 512]), mask=torch.Size([1, 1, 51, 51])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 51]); mask:torch.Size([1, 1, 51, 51])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 51, 512]), V=torch.Size([1, 51, 512]), mask=torch.Size([1, 1, 51, 51])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 51]); mask:torch.Size([1, 1, 51, 51])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 51, 512]), V=torch.Size([1, 51, 512]), mask=torch.Size([1, 1, 51, 51])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 51]); mask:torch.Size([1, 1, 51, 51])
in MultiheadAttention: Q=torch.Size([1, 51, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 51, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 52, 512]), V=torch.Size([1, 52, 512]), mask=torch.Size([1, 1, 52, 52])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 52]); mask:torch.Size([1, 1, 52, 52])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 52, 512]), V=torch.Size([1, 52, 512]), mask=torch.Size([1, 1, 52, 52])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 52]); mask:torch.Size([1, 1, 52, 52])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 52, 512]), V=torch.Size([1, 52, 512]), mask=torch.Size([1, 1, 52, 52])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 52]); mask:torch.Size([1, 1, 52, 52])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 52, 512]), V=torch.Size([1, 52, 512]), mask=torch.Size([1, 1, 52, 52])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 52]); mask:torch.Size([1, 1, 52, 52])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 52, 512]), V=torch.Size([1, 52, 512]), mask=torch.Size([1, 1, 52, 52])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 52]); mask:torch.Size([1, 1, 52, 52])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 52, 512]), V=torch.Size([1, 52, 512]), mask=torch.Size([1, 1, 52, 52])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 52]); mask:torch.Size([1, 1, 52, 52])
in MultiheadAttention: Q=torch.Size([1, 52, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 52, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 53, 512]), V=torch.Size([1, 53, 512]), mask=torch.Size([1, 1, 53, 53])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 53]); mask:torch.Size([1, 1, 53, 53])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 53, 512]), V=torch.Size([1, 53, 512]), mask=torch.Size([1, 1, 53, 53])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 53]); mask:torch.Size([1, 1, 53, 53])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 53, 512]), V=torch.Size([1, 53, 512]), mask=torch.Size([1, 1, 53, 53])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 53]); mask:torch.Size([1, 1, 53, 53])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 53, 512]), V=torch.Size([1, 53, 512]), mask=torch.Size([1, 1, 53, 53])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 53]); mask:torch.Size([1, 1, 53, 53])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 53, 512]), V=torch.Size([1, 53, 512]), mask=torch.Size([1, 1, 53, 53])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 53]); mask:torch.Size([1, 1, 53, 53])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 53, 512]), V=torch.Size([1, 53, 512]), mask=torch.Size([1, 1, 53, 53])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 53]); mask:torch.Size([1, 1, 53, 53])
in MultiheadAttention: Q=torch.Size([1, 53, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 53, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 54, 512]), V=torch.Size([1, 54, 512]), mask=torch.Size([1, 1, 54, 54])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 54]); mask:torch.Size([1, 1, 54, 54])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 54, 512]), V=torch.Size([1, 54, 512]), mask=torch.Size([1, 1, 54, 54])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 54]); mask:torch.Size([1, 1, 54, 54])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 54, 512]), V=torch.Size([1, 54, 512]), mask=torch.Size([1, 1, 54, 54])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 54]); mask:torch.Size([1, 1, 54, 54])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 54, 512]), V=torch.Size([1, 54, 512]), mask=torch.Size([1, 1, 54, 54])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 54]); mask:torch.Size([1, 1, 54, 54])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 54, 512]), V=torch.Size([1, 54, 512]), mask=torch.Size([1, 1, 54, 54])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 54]); mask:torch.Size([1, 1, 54, 54])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 54, 512]), V=torch.Size([1, 54, 512]), mask=torch.Size([1, 1, 54, 54])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 54]); mask:torch.Size([1, 1, 54, 54])
in MultiheadAttention: Q=torch.Size([1, 54, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 54, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 55, 512]), V=torch.Size([1, 55, 512]), mask=torch.Size([1, 1, 55, 55])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 55]); mask:torch.Size([1, 1, 55, 55])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 55, 512]), V=torch.Size([1, 55, 512]), mask=torch.Size([1, 1, 55, 55])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 55]); mask:torch.Size([1, 1, 55, 55])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 55, 512]), V=torch.Size([1, 55, 512]), mask=torch.Size([1, 1, 55, 55])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 55]); mask:torch.Size([1, 1, 55, 55])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 55, 512]), V=torch.Size([1, 55, 512]), mask=torch.Size([1, 1, 55, 55])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 55]); mask:torch.Size([1, 1, 55, 55])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 55, 512]), V=torch.Size([1, 55, 512]), mask=torch.Size([1, 1, 55, 55])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 55]); mask:torch.Size([1, 1, 55, 55])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 55, 512]), V=torch.Size([1, 55, 512]), mask=torch.Size([1, 1, 55, 55])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 55]); mask:torch.Size([1, 1, 55, 55])
in MultiheadAttention: Q=torch.Size([1, 55, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 55, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 56, 512]), V=torch.Size([1, 56, 512]), mask=torch.Size([1, 1, 56, 56])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 56]); mask:torch.Size([1, 1, 56, 56])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 56, 512]), V=torch.Size([1, 56, 512]), mask=torch.Size([1, 1, 56, 56])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 56]); mask:torch.Size([1, 1, 56, 56])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 56, 512]), V=torch.Size([1, 56, 512]), mask=torch.Size([1, 1, 56, 56])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 56]); mask:torch.Size([1, 1, 56, 56])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 56, 512]), V=torch.Size([1, 56, 512]), mask=torch.Size([1, 1, 56, 56])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 56]); mask:torch.Size([1, 1, 56, 56])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 56, 512]), V=torch.Size([1, 56, 512]), mask=torch.Size([1, 1, 56, 56])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 56]); mask:torch.Size([1, 1, 56, 56])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 56, 512]), V=torch.Size([1, 56, 512]), mask=torch.Size([1, 1, 56, 56])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 56]); mask:torch.Size([1, 1, 56, 56])
in MultiheadAttention: Q=torch.Size([1, 56, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 56, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 57, 512]), V=torch.Size([1, 57, 512]), mask=torch.Size([1, 1, 57, 57])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 57]); mask:torch.Size([1, 1, 57, 57])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 57, 512]), V=torch.Size([1, 57, 512]), mask=torch.Size([1, 1, 57, 57])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 57]); mask:torch.Size([1, 1, 57, 57])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 57, 512]), V=torch.Size([1, 57, 512]), mask=torch.Size([1, 1, 57, 57])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 57]); mask:torch.Size([1, 1, 57, 57])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 57, 512]), V=torch.Size([1, 57, 512]), mask=torch.Size([1, 1, 57, 57])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 57]); mask:torch.Size([1, 1, 57, 57])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 57, 512]), V=torch.Size([1, 57, 512]), mask=torch.Size([1, 1, 57, 57])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 57]); mask:torch.Size([1, 1, 57, 57])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 57, 512]), V=torch.Size([1, 57, 512]), mask=torch.Size([1, 1, 57, 57])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 57]); mask:torch.Size([1, 1, 57, 57])
in MultiheadAttention: Q=torch.Size([1, 57, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 57, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 58, 512]), V=torch.Size([1, 58, 512]), mask=torch.Size([1, 1, 58, 58])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 58]); mask:torch.Size([1, 1, 58, 58])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 58, 512]), V=torch.Size([1, 58, 512]), mask=torch.Size([1, 1, 58, 58])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 58]); mask:torch.Size([1, 1, 58, 58])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 58, 512]), V=torch.Size([1, 58, 512]), mask=torch.Size([1, 1, 58, 58])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 58]); mask:torch.Size([1, 1, 58, 58])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 58, 512]), V=torch.Size([1, 58, 512]), mask=torch.Size([1, 1, 58, 58])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 58]); mask:torch.Size([1, 1, 58, 58])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 58, 512]), V=torch.Size([1, 58, 512]), mask=torch.Size([1, 1, 58, 58])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 58]); mask:torch.Size([1, 1, 58, 58])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 58, 512]), V=torch.Size([1, 58, 512]), mask=torch.Size([1, 1, 58, 58])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 58]); mask:torch.Size([1, 1, 58, 58])
in MultiheadAttention: Q=torch.Size([1, 58, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 58, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 59, 512]), V=torch.Size([1, 59, 512]), mask=torch.Size([1, 1, 59, 59])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 59]); mask:torch.Size([1, 1, 59, 59])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 59, 512]), V=torch.Size([1, 59, 512]), mask=torch.Size([1, 1, 59, 59])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 59]); mask:torch.Size([1, 1, 59, 59])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 59, 512]), V=torch.Size([1, 59, 512]), mask=torch.Size([1, 1, 59, 59])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 59]); mask:torch.Size([1, 1, 59, 59])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 59, 512]), V=torch.Size([1, 59, 512]), mask=torch.Size([1, 1, 59, 59])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 59]); mask:torch.Size([1, 1, 59, 59])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 59, 512]), V=torch.Size([1, 59, 512]), mask=torch.Size([1, 1, 59, 59])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 59]); mask:torch.Size([1, 1, 59, 59])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 59, 512]), V=torch.Size([1, 59, 512]), mask=torch.Size([1, 1, 59, 59])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 59]); mask:torch.Size([1, 1, 59, 59])
in MultiheadAttention: Q=torch.Size([1, 59, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 59, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 60, 512]), V=torch.Size([1, 60, 512]), mask=torch.Size([1, 1, 60, 60])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 60]); mask:torch.Size([1, 1, 60, 60])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 60, 512]), V=torch.Size([1, 60, 512]), mask=torch.Size([1, 1, 60, 60])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 60]); mask:torch.Size([1, 1, 60, 60])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 60, 512]), V=torch.Size([1, 60, 512]), mask=torch.Size([1, 1, 60, 60])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 60]); mask:torch.Size([1, 1, 60, 60])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 60, 512]), V=torch.Size([1, 60, 512]), mask=torch.Size([1, 1, 60, 60])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 60]); mask:torch.Size([1, 1, 60, 60])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 60, 512]), V=torch.Size([1, 60, 512]), mask=torch.Size([1, 1, 60, 60])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 60]); mask:torch.Size([1, 1, 60, 60])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 60, 512]), V=torch.Size([1, 60, 512]), mask=torch.Size([1, 1, 60, 60])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 60]); mask:torch.Size([1, 1, 60, 60])
in MultiheadAttention: Q=torch.Size([1, 60, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 60, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 61, 512]), V=torch.Size([1, 61, 512]), mask=torch.Size([1, 1, 61, 61])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 61]); mask:torch.Size([1, 1, 61, 61])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 61, 512]), V=torch.Size([1, 61, 512]), mask=torch.Size([1, 1, 61, 61])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 61]); mask:torch.Size([1, 1, 61, 61])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 61, 512]), V=torch.Size([1, 61, 512]), mask=torch.Size([1, 1, 61, 61])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 61]); mask:torch.Size([1, 1, 61, 61])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 61, 512]), V=torch.Size([1, 61, 512]), mask=torch.Size([1, 1, 61, 61])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 61]); mask:torch.Size([1, 1, 61, 61])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 61, 512]), V=torch.Size([1, 61, 512]), mask=torch.Size([1, 1, 61, 61])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 61]); mask:torch.Size([1, 1, 61, 61])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 61, 512]), V=torch.Size([1, 61, 512]), mask=torch.Size([1, 1, 61, 61])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 61]); mask:torch.Size([1, 1, 61, 61])
in MultiheadAttention: Q=torch.Size([1, 61, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 61, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 62, 512]), V=torch.Size([1, 62, 512]), mask=torch.Size([1, 1, 62, 62])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 62]); mask:torch.Size([1, 1, 62, 62])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 62, 512]), V=torch.Size([1, 62, 512]), mask=torch.Size([1, 1, 62, 62])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 62]); mask:torch.Size([1, 1, 62, 62])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 62, 512]), V=torch.Size([1, 62, 512]), mask=torch.Size([1, 1, 62, 62])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 62]); mask:torch.Size([1, 1, 62, 62])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 62, 512]), V=torch.Size([1, 62, 512]), mask=torch.Size([1, 1, 62, 62])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 62]); mask:torch.Size([1, 1, 62, 62])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 62, 512]), V=torch.Size([1, 62, 512]), mask=torch.Size([1, 1, 62, 62])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 62]); mask:torch.Size([1, 1, 62, 62])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 62, 512]), V=torch.Size([1, 62, 512]), mask=torch.Size([1, 1, 62, 62])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 62]); mask:torch.Size([1, 1, 62, 62])
in MultiheadAttention: Q=torch.Size([1, 62, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 62, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 63, 512]), V=torch.Size([1, 63, 512]), mask=torch.Size([1, 1, 63, 63])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 63]); mask:torch.Size([1, 1, 63, 63])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 63, 512]), V=torch.Size([1, 63, 512]), mask=torch.Size([1, 1, 63, 63])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 63]); mask:torch.Size([1, 1, 63, 63])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 63, 512]), V=torch.Size([1, 63, 512]), mask=torch.Size([1, 1, 63, 63])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 63]); mask:torch.Size([1, 1, 63, 63])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 63, 512]), V=torch.Size([1, 63, 512]), mask=torch.Size([1, 1, 63, 63])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 63]); mask:torch.Size([1, 1, 63, 63])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 63, 512]), V=torch.Size([1, 63, 512]), mask=torch.Size([1, 1, 63, 63])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 63]); mask:torch.Size([1, 1, 63, 63])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 63, 512]), V=torch.Size([1, 63, 512]), mask=torch.Size([1, 1, 63, 63])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 63]); mask:torch.Size([1, 1, 63, 63])
in MultiheadAttention: Q=torch.Size([1, 63, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 63, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 64, 512]), V=torch.Size([1, 64, 512]), mask=torch.Size([1, 1, 64, 64])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 64]); mask:torch.Size([1, 1, 64, 64])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 64, 512]), V=torch.Size([1, 64, 512]), mask=torch.Size([1, 1, 64, 64])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 64]); mask:torch.Size([1, 1, 64, 64])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 64, 512]), V=torch.Size([1, 64, 512]), mask=torch.Size([1, 1, 64, 64])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 64]); mask:torch.Size([1, 1, 64, 64])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 64, 512]), V=torch.Size([1, 64, 512]), mask=torch.Size([1, 1, 64, 64])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 64]); mask:torch.Size([1, 1, 64, 64])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 64, 512]), V=torch.Size([1, 64, 512]), mask=torch.Size([1, 1, 64, 64])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 64]); mask:torch.Size([1, 1, 64, 64])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 64, 512]), V=torch.Size([1, 64, 512]), mask=torch.Size([1, 1, 64, 64])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 64]); mask:torch.Size([1, 1, 64, 64])
in MultiheadAttention: Q=torch.Size([1, 64, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 64, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 65, 512]), V=torch.Size([1, 65, 512]), mask=torch.Size([1, 1, 65, 65])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 65]); mask:torch.Size([1, 1, 65, 65])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 65, 512]), V=torch.Size([1, 65, 512]), mask=torch.Size([1, 1, 65, 65])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 65]); mask:torch.Size([1, 1, 65, 65])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 65, 512]), V=torch.Size([1, 65, 512]), mask=torch.Size([1, 1, 65, 65])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 65]); mask:torch.Size([1, 1, 65, 65])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 65, 512]), V=torch.Size([1, 65, 512]), mask=torch.Size([1, 1, 65, 65])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 65]); mask:torch.Size([1, 1, 65, 65])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 65, 512]), V=torch.Size([1, 65, 512]), mask=torch.Size([1, 1, 65, 65])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 65]); mask:torch.Size([1, 1, 65, 65])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 65, 512]), V=torch.Size([1, 65, 512]), mask=torch.Size([1, 1, 65, 65])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 65]); mask:torch.Size([1, 1, 65, 65])
in MultiheadAttention: Q=torch.Size([1, 65, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 65, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 66, 512]), V=torch.Size([1, 66, 512]), mask=torch.Size([1, 1, 66, 66])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 66]); mask:torch.Size([1, 1, 66, 66])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 66, 512]), V=torch.Size([1, 66, 512]), mask=torch.Size([1, 1, 66, 66])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 66]); mask:torch.Size([1, 1, 66, 66])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 66, 512]), V=torch.Size([1, 66, 512]), mask=torch.Size([1, 1, 66, 66])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 66]); mask:torch.Size([1, 1, 66, 66])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 66, 512]), V=torch.Size([1, 66, 512]), mask=torch.Size([1, 1, 66, 66])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 66]); mask:torch.Size([1, 1, 66, 66])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 66, 512]), V=torch.Size([1, 66, 512]), mask=torch.Size([1, 1, 66, 66])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 66]); mask:torch.Size([1, 1, 66, 66])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 66, 512]), V=torch.Size([1, 66, 512]), mask=torch.Size([1, 1, 66, 66])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 66]); mask:torch.Size([1, 1, 66, 66])
in MultiheadAttention: Q=torch.Size([1, 66, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 66, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 67, 512]), V=torch.Size([1, 67, 512]), mask=torch.Size([1, 1, 67, 67])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 67]); mask:torch.Size([1, 1, 67, 67])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 67, 512]), V=torch.Size([1, 67, 512]), mask=torch.Size([1, 1, 67, 67])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 67]); mask:torch.Size([1, 1, 67, 67])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 67, 512]), V=torch.Size([1, 67, 512]), mask=torch.Size([1, 1, 67, 67])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 67]); mask:torch.Size([1, 1, 67, 67])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 67, 512]), V=torch.Size([1, 67, 512]), mask=torch.Size([1, 1, 67, 67])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 67]); mask:torch.Size([1, 1, 67, 67])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 67, 512]), V=torch.Size([1, 67, 512]), mask=torch.Size([1, 1, 67, 67])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 67]); mask:torch.Size([1, 1, 67, 67])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 67, 512]), V=torch.Size([1, 67, 512]), mask=torch.Size([1, 1, 67, 67])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 67]); mask:torch.Size([1, 1, 67, 67])
in MultiheadAttention: Q=torch.Size([1, 67, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 67, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 68, 512]), V=torch.Size([1, 68, 512]), mask=torch.Size([1, 1, 68, 68])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 68]); mask:torch.Size([1, 1, 68, 68])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 68, 512]), V=torch.Size([1, 68, 512]), mask=torch.Size([1, 1, 68, 68])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 68]); mask:torch.Size([1, 1, 68, 68])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 68, 512]), V=torch.Size([1, 68, 512]), mask=torch.Size([1, 1, 68, 68])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 68]); mask:torch.Size([1, 1, 68, 68])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 68, 512]), V=torch.Size([1, 68, 512]), mask=torch.Size([1, 1, 68, 68])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 68]); mask:torch.Size([1, 1, 68, 68])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 68, 512]), V=torch.Size([1, 68, 512]), mask=torch.Size([1, 1, 68, 68])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 68]); mask:torch.Size([1, 1, 68, 68])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 68, 512]), V=torch.Size([1, 68, 512]), mask=torch.Size([1, 1, 68, 68])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 68]); mask:torch.Size([1, 1, 68, 68])
in MultiheadAttention: Q=torch.Size([1, 68, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 68, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 69, 512]), V=torch.Size([1, 69, 512]), mask=torch.Size([1, 1, 69, 69])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 69]); mask:torch.Size([1, 1, 69, 69])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 69, 512]), V=torch.Size([1, 69, 512]), mask=torch.Size([1, 1, 69, 69])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 69]); mask:torch.Size([1, 1, 69, 69])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 69, 512]), V=torch.Size([1, 69, 512]), mask=torch.Size([1, 1, 69, 69])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 69]); mask:torch.Size([1, 1, 69, 69])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 69, 512]), V=torch.Size([1, 69, 512]), mask=torch.Size([1, 1, 69, 69])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 69]); mask:torch.Size([1, 1, 69, 69])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 69, 512]), V=torch.Size([1, 69, 512]), mask=torch.Size([1, 1, 69, 69])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 69]); mask:torch.Size([1, 1, 69, 69])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 69, 512]), V=torch.Size([1, 69, 512]), mask=torch.Size([1, 1, 69, 69])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 69]); mask:torch.Size([1, 1, 69, 69])
in MultiheadAttention: Q=torch.Size([1, 69, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 69, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 70, 512]), V=torch.Size([1, 70, 512]), mask=torch.Size([1, 1, 70, 70])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 70]); mask:torch.Size([1, 1, 70, 70])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 70, 512]), V=torch.Size([1, 70, 512]), mask=torch.Size([1, 1, 70, 70])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 70]); mask:torch.Size([1, 1, 70, 70])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 70, 512]), V=torch.Size([1, 70, 512]), mask=torch.Size([1, 1, 70, 70])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 70]); mask:torch.Size([1, 1, 70, 70])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 70, 512]), V=torch.Size([1, 70, 512]), mask=torch.Size([1, 1, 70, 70])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 70]); mask:torch.Size([1, 1, 70, 70])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 70, 512]), V=torch.Size([1, 70, 512]), mask=torch.Size([1, 1, 70, 70])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 70]); mask:torch.Size([1, 1, 70, 70])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 70, 512]), V=torch.Size([1, 70, 512]), mask=torch.Size([1, 1, 70, 70])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 70]); mask:torch.Size([1, 1, 70, 70])
in MultiheadAttention: Q=torch.Size([1, 70, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 70, 128]); mask:torch.Size([1, 1, 1, 128])
in greedy_decode: during DECODING
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 71, 512]), V=torch.Size([1, 71, 512]), mask=torch.Size([1, 1, 71, 71])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 71]); mask:torch.Size([1, 1, 71, 71])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 71, 512]), V=torch.Size([1, 71, 512]), mask=torch.Size([1, 1, 71, 71])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 71]); mask:torch.Size([1, 1, 71, 71])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 71, 512]), V=torch.Size([1, 71, 512]), mask=torch.Size([1, 1, 71, 71])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 71]); mask:torch.Size([1, 1, 71, 71])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 71, 512]), V=torch.Size([1, 71, 512]), mask=torch.Size([1, 1, 71, 71])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 71]); mask:torch.Size([1, 1, 71, 71])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 71, 512]), V=torch.Size([1, 71, 512]), mask=torch.Size([1, 1, 71, 71])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 71]); mask:torch.Size([1, 1, 71, 71])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 128]); mask:torch.Size([1, 1, 1, 128])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 71, 512]), V=torch.Size([1, 71, 512]), mask=torch.Size([1, 1, 71, 71])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 71]); mask:torch.Size([1, 1, 71, 71])
in MultiheadAttention: Q=torch.Size([1, 71, 512]), K=torch.Size([1, 128, 512]), V=torch.Size([1, 128, 512]), mask=torch.Size([1, 1, 1, 128])
after  torch.matmul(query, key.transpose(-2, -1)) : scores: torch.Size([1, 8, 71, 128]); mask:torch.Size([1, 1, 1, 128])
